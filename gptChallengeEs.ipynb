{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eca6be9-3ab2-4756-80a0-4880568d1cd7",
   "metadata": {},
   "source": [
    "# GPTChallenge: diagn√≥stico a partir de HCE\n",
    "\n",
    "**Grupo 2: GPTSovereigns** üë®‚Äçüíª\n",
    "\n",
    "Integrantes: \n",
    "- Mart√≠nez Leal, Jes√∫s\n",
    "- Ortega Mediavilla, Samuel\n",
    "- Vicente Mart√≠nez, Pablo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff556ab",
   "metadata": {},
   "source": [
    "**OBJETIVO**. Clasificar texto procedente de historias cl√≠nicas electr√≥nicas en c√≥digo CIE-10 usando ChatGPT.\n",
    "\n",
    "El CIE-10 (Clasificaci√≥n Internacional de Enfermedades, d√©cima edici√≥n) es un sistema de clasificaci√≥n m√©dica utilizado para codificar y clasificar enfermedades y otros problemas de salud. ASigna c√≥digos alfanum√©ricos a una amplia variedad de enfermedades, trastornos, lesiones, causas externas de morbilidad y otros problemas de salud. Cada c√≥digo est√° compuesto por entre 3 y 7 caracteres, que proporcionan informaci√≥n detallada sobre la condici√≥n m√©dica o el evento en cuesti√≥n.\n",
    "\n",
    "Tendremos un **conjunto de train** con historias cl√≠nicas electr√≥nicas y varias etiquetas CIE-10 asociadas, as√≠ como un **conjunto de test** *no etiquetado*. El resultado de la predicci√≥n solo podr√° ser visto al subirlo a una p√°gina web que nos brindaron los profesores, disponiendo de un m√°ximo de **3 intentos**. Para ello, previamente deberemos guardar las predicciones sobre este conjunto en un **dataframe** que exportaremos a formato `.csv`.\n",
    "\n",
    "Ser√° necesario que demos tambi√©n nuestro historial de chat en ChatGPT (uno por integrante) en lo relacionado con el proyecto, por lo que ser√° necesario dar el enlace directo de este (al final del notebook est√°).\n",
    "\n",
    "Por √∫ltimo, se deber√° escribir un art√≠culo cient√≠fico con las secciones siguientes: 1. Introducci√≥n, 2. Materiales y m√©todos, 3. Resultados, 4. Discusi√≥n y 5. Conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5ba28",
   "metadata": {},
   "source": [
    "Aunque el dataset de **train** tiene casi 1000 c√≥digos, tomaremos solo las historias cl√≠nicas con los **10 c√≥digos m√°s frecuentes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d9698",
   "metadata": {},
   "source": [
    "=============================================================================================================================================================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de358c45-755b-4fad-9d68-72e591de8b4b",
   "metadata": {},
   "source": [
    "Vamos a trabajar con el corpus **CodEsp** (textos de historial cl√≠nico etiquetados con sus c√≥digos CIE-10 Diagn√≥stico). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdaf085",
   "metadata": {},
   "source": [
    "## Carga de librer√≠as y definici√≥n de funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2d4da20-1cea-4309-941f-c2440b01801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # used for data manipulation\n",
    "import os, re, spacy # used for text processing, regular expressions, and nlp purposes\n",
    "import numpy as np # used for numerical operations\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # used for encoding labels\n",
    "from sklearn.feature_extraction.text import CountVectorizer # used for text vectorization\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier # used for classification\n",
    "from sklearn.neighbors import KNeighborsClassifier # used for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier # used for classification, ensemble methods\n",
    "from sklearn.multioutput import MultiOutputClassifier # used for multi-label classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # used for splitting data, hyperparameter tuning\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score # used for evaluation\n",
    "from sklearn.metrics.pairwise import cosine_similarity # used for similarity calculation\n",
    "\n",
    "import lightgbm as lgb # used for classification\n",
    "from scipy.sparse import vstack # used for stacking sparse matrices\n",
    "\n",
    "pd.options.display.max_colwidth = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d476c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3 # semilla uwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc9e5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDoc(nlp, doc, nMinCharacters = 0):\n",
    "    \"\"\"\n",
    "    Normaliza un texto eliminando palabras por debajo del m√≠nimo de caracteres, stop words y n√∫meros.\n",
    "    Para ello, tokeniza empleando un modelo de Spacy.\n",
    "    \"\"\"\n",
    "    # Separar en tokens\n",
    "    tokens = nlp(doc)\n",
    "    # Filtrar tokens\n",
    "    filtered_tokens = [t.lower_ for t in tokens if (len(t.text) >= nMinCharacters) and not t.is_punct and not re.match('[0-9]+', t.text)] # Filtrar palabras por longitud y quitar n√∫meros y signos de puntuaci√≥n\n",
    "    # Recombinamos los tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def findMostSimilar(similarityDf: pd.DataFrame, data: pd.DataFrame, nMostSimilars: int = 1):\n",
    "    \"\"\"\n",
    "    Encuentra la etiqueta de los documentos m√°s similares.\n",
    "    \"\"\"\n",
    "    # Crear df de resultados\n",
    "    results = pd.DataFrame(index = similarityDf.index, columns = ['archivoMostSimilar', 'similarity', 'codigosPred'])\n",
    "\n",
    "    for index, row in similarityDf.iterrows():\n",
    "        \n",
    "        # Buscar la m√°xima similitud\n",
    "        mostSimilar = row.nlargest(nMostSimilars)\n",
    "        \n",
    "        results.loc[index, 'archivoMostSimilar'] = mostSimilar.index.values\n",
    "        if nMostSimilars == 1:\n",
    "            results.loc[index, 'similarity'] = mostSimilar.values[0]\n",
    "        else:\n",
    "            results.loc[index, 'similarity'] = row[mostSimilar.index]\n",
    "\n",
    "    # Coger las etiquetas\n",
    "    if nMostSimilars == 1:\n",
    "        results['codigosPred'] = data.loc[np.squeeze(np.vstack(results['archivoMostSimilar'].values)), 'codigos'].values\n",
    "    else:\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "def checkAccuracy(pred: pd.DataFrame, data: pd.DataFrame):\n",
    "    pred['codigos'] = data.loc[pred.index, 'codigos']\n",
    "\n",
    "    pred['guess'] = pred.apply(\n",
    "        lambda row: len(set(row['codigosPred']).intersection(row['codigos'])) / len(row['codigosPred']),\n",
    "        axis = 1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e8803",
   "metadata": {},
   "source": [
    "## Lectura y preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c8c08",
   "metadata": {},
   "source": [
    "### Conjunto de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "566a8909-46d3-4ccd-8a67-96d819f60e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8316 entries, 0 to 8315\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   archivo  8316 non-null   object\n",
      " 1   codigo   8316 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 130.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#los c√≥digos est√°n en un TSV con un c√≥digo por l√≠nea\n",
    "train_diag = pd.read_csv(\"data/train/train.tsv\", sep=\"\\t\", header=None, names=[\"archivo\", \"codigo\"])\n",
    "train_diag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3cbd33aa-1d54-419f-a1f1-728bb03b79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "r52    163\n",
      "r10    163\n",
      "r59    160\n",
      "r69    150\n",
      "r50    144\n",
      "      ... \n",
      "c31      1\n",
      "d62      1\n",
      "s53      1\n",
      "s34      1\n",
      "n81      1\n",
      "Name: count, Length: 918, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cogemos la categor√≠a superior de cada c√≥digo y las agrupamos\n",
    "train_diag['cat'] = train_diag['codigo'].str.extract(r'(\\w\\d\\d)')\n",
    "print(train_diag['cat'].value_counts())\n",
    "train_diag['cat'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8eaf143a-b460-410e-8af2-a268181153d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r52', 'r10', 'r59', 'r69', 'r50', 'r60', 'i10', 'r11', 'n28', 'd49']\n"
     ]
    }
   ],
   "source": [
    "categories=train_diag['cat'].value_counts()[:10] # cogemos las 10 categor√≠as m√°s comunes\n",
    "top_categorias = categories.index.to_list()\n",
    "print(top_categorias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "25e458c1-01ec-4c48-8c3e-45f37e4553c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seleccionamos s√≥lo las etiquetas de este subconjunto\n",
    "train_diag = train_diag[np.isin(train_diag['cat'], top_categorias)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c1befd52-7347-4e6d-8760-3a08b5fd8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 562 entries, S0004-06142005000700014-1 to S2340-98942015000100005-1\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   texto    562 non-null    object\n",
      " 1   codigos  562 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 13.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#cargamos los dos conjuntos de train\n",
    "path = 'data/train/text_files/'\n",
    "\n",
    "corpus = []\n",
    "for f in [f for f in os.listdir(path) if f.endswith('.txt')]:\n",
    "    with open(os.path.join(path, f), encoding=\"utf8\") as text:\n",
    "        texto = text.read()\n",
    "    #buscamos c√≥digos\n",
    "    file = f[:-4]\n",
    "    codigos = train_diag.query('archivo==@file')['cat'].to_list()\n",
    "    codigos = list(set(codigos))\n",
    "    if codigos:\n",
    "        corpus.append({\n",
    "            'archivo': file,\n",
    "            'texto': texto,\n",
    "            'codigos': codigos\n",
    "        })\n",
    "    \n",
    "df_train = pd.DataFrame(corpus).set_index('archivo')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dcf73334-972c-4746-829f-0891641a1b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>codigos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>archivo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0211-69952013000200019-2</th>\n",
       "      <td>Var√≥n de 45 a√±os afecto de ERC estadio 5 secundaria a poliquistosis hepatorrenal con antecedente de hipertensi√≥n arterial e hiperuricemia. Ante el deterioro progresivo de funci√≥n renal con necesidad de tratamiento renal sustitutivo, y tras la explicaci√≥n de las diferentes t√©cnicas, se inserta cat√©ter peritoneal recto no autoposicionante de 1 cuff. Un mes despu√©s tras inicio de DPCA en su domicilio con 4 intercambios de 2 litros de glucosa 1,36 %, el paciente acude a la Unidad de Di√°lisis Peritoneal, refiriendo edematizaci√≥n inguinoescrotal de 48 h de evoluci√≥n. Tras descartar orquiepididimitis, se efect√∫a TAC-peritoneograf√≠a, como en el caso anterior, confirmando el paso de contraste a test√≠culos a trav√©s de un proceso vaginal permeable. Se decide reposo peritoneal y transferencia a hemodi√°lisis. Un mes despu√©s se procede al cierre quir√∫rgico del conducto peritoneovaginal, reiniciando DPCA en junio de 2009 sin incidencias ni reaparici√≥n de la fuga.\\n\\n</td>\n",
       "      <td>[n28, r60, i10]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            texto  \\\n",
       "archivo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "S0211-69952013000200019-2  Var√≥n de 45 a√±os afecto de ERC estadio 5 secundaria a poliquistosis hepatorrenal con antecedente de hipertensi√≥n arterial e hiperuricemia. Ante el deterioro progresivo de funci√≥n renal con necesidad de tratamiento renal sustitutivo, y tras la explicaci√≥n de las diferentes t√©cnicas, se inserta cat√©ter peritoneal recto no autoposicionante de 1 cuff. Un mes despu√©s tras inicio de DPCA en su domicilio con 4 intercambios de 2 litros de glucosa 1,36 %, el paciente acude a la Unidad de Di√°lisis Peritoneal, refiriendo edematizaci√≥n inguinoescrotal de 48 h de evoluci√≥n. Tras descartar orquiepididimitis, se efect√∫a TAC-peritoneograf√≠a, como en el caso anterior, confirmando el paso de contraste a test√≠culos a trav√©s de un proceso vaginal permeable. Se decide reposo peritoneal y transferencia a hemodi√°lisis. Un mes despu√©s se procede al cierre quir√∫rgico del conducto peritoneovaginal, reiniciando DPCA en junio de 2009 sin incidencias ni reaparici√≥n de la fuga.\\n\\n   \n",
       "\n",
       "                                   codigos  \n",
       "archivo                                     \n",
       "S0211-69952013000200019-2  [n28, r60, i10]  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc77de8-8256-4653-9de0-690dacc58095",
   "metadata": {},
   "source": [
    "### Conjunto de test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a6ad4",
   "metadata": {},
   "source": [
    "Repetimos algo similar, pero ahora no tenemos las etiquetas disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d4d6ea92-47fd-462a-9067-33fdfdfd31a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192 entries, 0 to 191\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   archivo  192 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#los c√≥digos est√°n en un TSV con un c√≥digo por l√≠nea\n",
    "test_diag = pd.read_csv(\"data/test/test.tsv\", sep = \"\\t\", header = None, names = [\"archivo\"])\n",
    "test_diag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "80ff5875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 192 entries, S0004-06142005000500011-1 to S2254-28842014000300010-1\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   texto   192 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "path = 'data/test/text_files/'\n",
    "\n",
    "corpus = []\n",
    "for f in [f for f in os.listdir(path) if f.endswith('.txt')]:\n",
    "    file = os.path.splitext(f)[0]\n",
    "    if file in test_diag['archivo'].values:\n",
    "        with open(os.path.join(path, f), encoding=\"utf8\") as text:\n",
    "            texto = text.read()\n",
    "            corpus.append({\n",
    "                'archivo': file,\n",
    "                'texto': texto\n",
    "            })\n",
    "    \n",
    "df_test = pd.DataFrame(corpus).set_index('archivo')\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ae159e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>archivo</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S1137-66272014000300016-1</th>\n",
       "      <td>Mujer de 36 a√±os sin antecedentes m√©dicos o epidemiol√≥gicos de inter√©s que es ingresada en la Unidad de Cuidados Intensivos por sepsis y meningitis por meningococo tipo B aislado en cultivos sangu√≠neos y de l√≠quido cefalorraqu√≠deo. Recibi√≥ soporte fluidoter√°pico, ceftriaxona y vancomicina durante 10 d√≠as con desaparici√≥n de par√°metros cl√≠nicos y anal√≠ticos de sepsis aunque con persistencia de fiebre diaria de 38o vespertina bien tolerada, sin otra focalidad infecciosa en las pruebas complementarias que inclu√≠an: hemocultivos seriados, urocultivo, radiograf√≠a de t√≥rax, ecograf√≠a abdominal y TAC de cr√°neo.\\nIngresa en planta de medicina interna para completar estudio y evoluci√≥n; permanece en estabilidad hemodin√°mica y buen estado general, fiebre de hasta 38,5oC bien tolerada, acceso venoso central subclavio derecho sin datos de infecci√≥n, escasas petequias en extremidades inferiores en fase de regresi√≥n siendo el resto de la exploraci√≥n general y neurol√≥gica normal.\\nEl hemograma, la hemostasia, el perfil renal, hep√°tico, tiroideo y lip√≠dico fueron normales, destacando un valor de ferritina de 775 ¬µg/dl, VSG 90 mm1a hora, la PCR en 23,8 mg/dl (VN &lt;0,3 mg/dl) y procalcitonina en 0,22 ng/ml. La serolog√≠a a virus de la hepatitis B y C, VIH, VEB, CMV, VHS tipos 1 y 2, Coxiella burnetti y Rickettsias fue negativa; autoinmunidad con ANA, ANCA, factor reumatoide, inmunoglobulinas y complementos fueron normales o negativos. Reacci√≥n de Mantoux negativa; hemocultivos a trav√©s de cat√©ter central, sangre perif√©rica y urocultivos fueron reiteradamente negativos; el l√≠quido cefalorraqu√≠deo de control era de caracter√≠sticas normales excepto una proteinorraquia de 65 mg/dl con baciloscopia y cultivo tambi√©n negativos. El fondo de ojo, electrocardiograma, ecocardiograf√≠a y la TAC t√≥raco-abdominal con contraste tambi√©n resultaron normales.\\nSe solicit√≥ una RMN craneal con contraste donde se apreciaba una discreta captaci√≥n men√≠ngea generalizada y m√∫ltiples lesiones de entre 7-10 mm, hipointensas en T1 e hiperintensas en T2 y Flair, en sustancia blanca temporal, en c√°psula externa izquierda, periventricular derecha, frontal y parietal anterior bilateral con marcada captaci√≥n perif√©rica de contraste en anillo y edema perilesional asociado. Ante los hallazgos compatibles con abscesos cerebrales m√∫ltiples secundarios a la meningococemia, se realiz√≥ una extensa b√∫squeda bibliogr√°fica (gu√≠as pr√°ctica cl√≠nica, revisiones sistem√°ticas, bases de datos bibliogr√°ficas, metabuscadores y revistas no indexadas relacionadas) que confirm√≥ los escasos datos reportados con respecto a dicha complicaci√≥n. En este punto se decidi√≥ la realizaci√≥n de un PET-TAC con el objetivo de descartar otros posibles hallazgos/focos que justificasen la persistencia del cuadro febril, corroborando dicha prueba la captaci√≥n hipermetab√≥lica de las lesiones cerebrales descritas, y ausencia de captaci√≥n a otro nivel corporal.\\n\\nDescart√°ndose otro foco infeccioso/inflamatorio, se mantuvo el tratamiento con ceftriaxona parenteral durante 8 semanas, con desaparici√≥n lenta pero progresiva de la fiebre, descenso paulatino hasta la normalizaci√≥n de PCR y VSG, as√≠ como desaparici√≥n de abscesos cerebrales en control de RMN tras finalizar tratamiento, sin presentar complicaciones ni recidiva del cuadro en el seguimiento ambulatorio posterior.\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   texto\n",
       "archivo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "S1137-66272014000300016-1  Mujer de 36 a√±os sin antecedentes m√©dicos o epidemiol√≥gicos de inter√©s que es ingresada en la Unidad de Cuidados Intensivos por sepsis y meningitis por meningococo tipo B aislado en cultivos sangu√≠neos y de l√≠quido cefalorraqu√≠deo. Recibi√≥ soporte fluidoter√°pico, ceftriaxona y vancomicina durante 10 d√≠as con desaparici√≥n de par√°metros cl√≠nicos y anal√≠ticos de sepsis aunque con persistencia de fiebre diaria de 38o vespertina bien tolerada, sin otra focalidad infecciosa en las pruebas complementarias que inclu√≠an: hemocultivos seriados, urocultivo, radiograf√≠a de t√≥rax, ecograf√≠a abdominal y TAC de cr√°neo.\\nIngresa en planta de medicina interna para completar estudio y evoluci√≥n; permanece en estabilidad hemodin√°mica y buen estado general, fiebre de hasta 38,5oC bien tolerada, acceso venoso central subclavio derecho sin datos de infecci√≥n, escasas petequias en extremidades inferiores en fase de regresi√≥n siendo el resto de la exploraci√≥n general y neurol√≥gica normal.\\nEl hemograma, la hemostasia, el perfil renal, hep√°tico, tiroideo y lip√≠dico fueron normales, destacando un valor de ferritina de 775 ¬µg/dl, VSG 90 mm1a hora, la PCR en 23,8 mg/dl (VN <0,3 mg/dl) y procalcitonina en 0,22 ng/ml. La serolog√≠a a virus de la hepatitis B y C, VIH, VEB, CMV, VHS tipos 1 y 2, Coxiella burnetti y Rickettsias fue negativa; autoinmunidad con ANA, ANCA, factor reumatoide, inmunoglobulinas y complementos fueron normales o negativos. Reacci√≥n de Mantoux negativa; hemocultivos a trav√©s de cat√©ter central, sangre perif√©rica y urocultivos fueron reiteradamente negativos; el l√≠quido cefalorraqu√≠deo de control era de caracter√≠sticas normales excepto una proteinorraquia de 65 mg/dl con baciloscopia y cultivo tambi√©n negativos. El fondo de ojo, electrocardiograma, ecocardiograf√≠a y la TAC t√≥raco-abdominal con contraste tambi√©n resultaron normales.\\nSe solicit√≥ una RMN craneal con contraste donde se apreciaba una discreta captaci√≥n men√≠ngea generalizada y m√∫ltiples lesiones de entre 7-10 mm, hipointensas en T1 e hiperintensas en T2 y Flair, en sustancia blanca temporal, en c√°psula externa izquierda, periventricular derecha, frontal y parietal anterior bilateral con marcada captaci√≥n perif√©rica de contraste en anillo y edema perilesional asociado. Ante los hallazgos compatibles con abscesos cerebrales m√∫ltiples secundarios a la meningococemia, se realiz√≥ una extensa b√∫squeda bibliogr√°fica (gu√≠as pr√°ctica cl√≠nica, revisiones sistem√°ticas, bases de datos bibliogr√°ficas, metabuscadores y revistas no indexadas relacionadas) que confirm√≥ los escasos datos reportados con respecto a dicha complicaci√≥n. En este punto se decidi√≥ la realizaci√≥n de un PET-TAC con el objetivo de descartar otros posibles hallazgos/focos que justificasen la persistencia del cuadro febril, corroborando dicha prueba la captaci√≥n hipermetab√≥lica de las lesiones cerebrales descritas, y ausencia de captaci√≥n a otro nivel corporal.\\n\\nDescart√°ndose otro foco infeccioso/inflamatorio, se mantuvo el tratamiento con ceftriaxona parenteral durante 8 semanas, con desaparici√≥n lenta pero progresiva de la fiebre, descenso paulatino hasta la normalizaci√≥n de PCR y VSG, as√≠ como desaparici√≥n de abscesos cerebrales en control de RMN tras finalizar tratamiento, sin presentar complicaciones ni recidiva del cuadro en el seguimiento ambulatorio posterior.\\n\\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472f280-db6a-438d-8340-f7b19758399b",
   "metadata": {},
   "source": [
    "### Binarizar las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae41be51-448e-433b-b10f-b4236afbaa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# para entrenar un clasificador multi-etiqueta generamos una matriz binaria de las etiquetas\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(df_train['codigos'])\n",
    "\n",
    "#Guardamos las clases utilizadas en el conjunto de train\n",
    "clases = mlb.classes_\n",
    "num_classes = clases.shape\n",
    "print(num_classes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f59c35",
   "metadata": {},
   "source": [
    "## Procesamiento del lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de409c67",
   "metadata": {},
   "source": [
    "En esta secci√≥n se llevar√° a cabo el procesamiento de los textos provenientes de las historias cl√≠nicas usando t√©cnicas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f1e21",
   "metadata": {},
   "source": [
    "Con la librer√≠a de spacy cargamos el modelo `es_core_news_lg` de Spacy. El sufijo *lg* indica que es un modelo de tama√±o grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "53ed5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download es_core_news_lg\n",
    "nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdde5f",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e3c4d",
   "metadata": {},
   "source": [
    "El enfoque Bag of Words (BoW) es una t√©cnica simple pero efectiva utilizada en el procesamiento de lenguaje natural para representar documentos de texto como vectores num√©ricos. La idea detr√°s de BoW es tratar cada documento como un \"saco\" de palabras, sin tener en cuenta el orden en que aparecen las palabras en el texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb61cb",
   "metadata": {},
   "source": [
    "Obtenemos resultados ligeramente mejores sin realizar un preprocesamiento con la funci√≥n `normalizeDoc()` previamente a la aplicaci√≥n del *Bag of Words*. La adici√≥n de *n-grams* empeora los resultados de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eb424683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_trainText = [normalizeDoc(nlp, doc) for doc in df_train['texto'].values]\n",
    "x_trainText = df_train['texto'].values\n",
    "x_testText = df_test['texto'].values\n",
    "\n",
    "vectorizer = CountVectorizer() # para el bag of words, de sklearn\n",
    "x_trainArray = vectorizer.fit_transform(x_trainText) # matriz tipo sparse\n",
    "x_testArray = vectorizer.transform(x_testText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7b747",
   "metadata": {},
   "source": [
    "Las matrices sparse `x_trainArray` y `x_testArray` contienen los recuentos de palabras del BoW para train y test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b0d0d",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e74155",
   "metadata": {},
   "source": [
    "Empezamos dividiendo nuestro conjunto de training en dos subconjuntos, que nos permitan distinguir entre lo que ser√≠a el conjunto de training y el de testing. Estrictamente, es un conjunto de validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9f9be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train_train, y_train_test = train_test_split(x_trainArray, y_train, test_size = 0.1, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d49be3",
   "metadata": {},
   "source": [
    "### Nuestro mejor modelo: MultiOutput con GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f992",
   "metadata": {},
   "source": [
    "Se aplica un modelo de clasificaci√≥n conocido como **Gradient Boosting Classifier**, el cual es una t√©cnica de ensemble learning que combina m√∫ltiples modelos de √°rboles de decisi√≥n d√©biles para construir un modelo m√°s robusto y preciso. Este modelo se caracteriza por ajustar secuencialmente nuevos √°rboles de decisi√≥n a los residuos del modelo anterior, lo que permite mejorar gradualmente la predicci√≥n. En este caso particular, se utilizar√° una variante del Gradient Boosting Classifier con una funci√≥n de p√©rdida exponencial y se configurar√°n varios hiperpar√°metros, como el n√∫mero de estimadores, la profundidad m√°xima del √°rbol y el n√∫mero m√≠nimo de muestras requeridas para dividir un nodo.\n",
    "\n",
    "Con el **MultiOutputClassifier** tendremos un clasificador por target. Esta es una estrategia simple para extender clasificadores que de manera nativa no soportan una clasificaci√≥n multi-output.\n",
    "\n",
    " Una vez que el modelo est√© entrenado, se evaluar√° su rendimiento utilizando m√©tricas como la precisi√≥n, el recall y el puntaje F1 a trav√©s de un informe de clasificaci√≥n.\n",
    "\n",
    "Informaci√≥n `GradientBoostingClassifier()`: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html.\n",
    "\n",
    "Informaci√≥n `MultiOutputClassifier()`: https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8f2927cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505, 17317)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f35a7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.62      0.73        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.80      0.57      0.67        14\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      0.93      0.97        15\n",
      "           6       1.00      0.31      0.47        13\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       1.00      1.00      1.00        17\n",
      "\n",
      "   micro avg       0.96      0.80      0.87       124\n",
      "   macro avg       0.96      0.80      0.85       124\n",
      "weighted avg       0.96      0.80      0.85       124\n",
      " samples avg       0.92      0.82      0.85       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = GradientBoostingClassifier(loss = 'exponential', n_estimators = 50, max_depth = 5, min_samples_split = 5, random_state = seed)\n",
    "model = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "model.fit(x_train, y_train_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0) # zero_division = 0 para evitar warnings\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c3f68",
   "metadata": {},
   "source": [
    "Vemos que obtenemos un f1-score de 0.85 en la versi√≥n `weighted` para el conjunto de validaci√≥n que hab√≠amos previamente establecido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11916ec",
   "metadata": {},
   "source": [
    "### Coseno similitud (*manual*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaff3c0",
   "metadata": {},
   "source": [
    "En esta subsecci√≥n se usa una t√©cnica de similitud coseno manual para encontrar los documentos m√°s similares dentro del conjunto de entrenamiento en relaci√≥n con los documentos del conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c275e5",
   "metadata": {},
   "source": [
    "Hacemos primeramente una divisi√≥n para los textos tambi√©n, al igual que hicimos anteriormente con las matrices sparse (mantenemos mismo random_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8476ec88-d3f6-477f-8cc9-d298ce8077b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTag = df_train.index.values \n",
    "\n",
    "x_trainT, x_testT, xTag_train, xTag_test = train_test_split(x_trainText, xTag, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0689372a-2ed7-45d5-895c-92b4f773c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x_trainVec = vectorizer.fit_transform(x_trainT)\n",
    "x_trainVecDf = pd.DataFrame(x_trainVec.toarray(), columns = vectorizer.get_feature_names_out(), index = xTag_train)\n",
    "\n",
    "x_testVec = vectorizer.transform(x_testT)\n",
    "x_testVecDf = pd.DataFrame(x_testVec.toarray(), columns = vectorizer.get_feature_names_out(), index = xTag_test)\n",
    "\n",
    "similarity = cosine_similarity(x_testVecDf, x_trainVecDf)\n",
    "similarityDf = pd.DataFrame(similarity, index = x_testVecDf.index, columns = x_trainVecDf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c8e24959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archivoMostSimilar</th>\n",
       "      <th>similarity</th>\n",
       "      <th>codigosPred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S0210-48062010000100019-4</th>\n",
       "      <td>S0210-48062009000900017-1</td>\n",
       "      <td>0.723986</td>\n",
       "      <td>[d49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1134-80462015000200005-1</th>\n",
       "      <td>S0376-78922016000200012-1</td>\n",
       "      <td>0.877009</td>\n",
       "      <td>[d49, i10, r69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1139-76322009000400007-1</th>\n",
       "      <td>S1698-69462006000400005-1</td>\n",
       "      <td>0.829529</td>\n",
       "      <td>[n28, d49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S0376-78922014000100013-1</th>\n",
       "      <td>S0376-78922016000200012-1</td>\n",
       "      <td>0.878946</td>\n",
       "      <td>[d49, i10, r69]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S1130-01082007001100012-1</th>\n",
       "      <td>S0212-16112010000100017-1</td>\n",
       "      <td>0.690295</td>\n",
       "      <td>[r60]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  archivoMostSimilar similarity  \\\n",
       "S0210-48062010000100019-4  S0210-48062009000900017-1   0.723986   \n",
       "S1134-80462015000200005-1  S0376-78922016000200012-1   0.877009   \n",
       "S1139-76322009000400007-1  S1698-69462006000400005-1   0.829529   \n",
       "S0376-78922014000100013-1  S0376-78922016000200012-1   0.878946   \n",
       "S1130-01082007001100012-1  S0212-16112010000100017-1   0.690295   \n",
       "\n",
       "                               codigosPred  \n",
       "S0210-48062010000100019-4            [d49]  \n",
       "S1134-80462015000200005-1  [d49, i10, r69]  \n",
       "S1139-76322009000400007-1       [n28, d49]  \n",
       "S0376-78922014000100013-1  [d49, i10, r69]  \n",
       "S1130-01082007001100012-1            [r60]  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostSimilarDf = findMostSimilar(similarityDf, df_train)\n",
    "\n",
    "mostSimilarDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f1d8d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Al menos 1 coincidencia: 0.4397163120567376\n",
      "Accuracy media: 0.2504728132387707\n"
     ]
    }
   ],
   "source": [
    "mostSimilarDf = checkAccuracy(mostSimilarDf, df_train)\n",
    "print(f\"Al menos 1 coincidencia: {(mostSimilarDf['guess'] != 0).sum() / mostSimilarDf.shape[0]}\")\n",
    "print(f\"Accuracy media: {mostSimilarDf['guess'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651e603",
   "metadata": {},
   "source": [
    "### Otros modelos de clasificaci√≥n multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef20299",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da121c0",
   "metadata": {},
   "source": [
    "El modelo de Random Forest es una t√©cnica de aprendizaje autom√°tico que se basa en la construcci√≥n de m√∫ltiples √°rboles de decisi√≥n durante el proceso de entrenamiento y combina sus predicciones para obtener una predicci√≥n final. Cada √°rbol en el bosque se entrena de forma independiente utilizando un subconjunto aleatorio de las caracter√≠sticas y las muestras del conjunto de entrenamiento, lo que fomenta la diversidad entre los √°rboles y ayuda a reducir el sobreajuste.\n",
    "\n",
    "Una de las caracter√≠sticas clave del modelo de Random Forest es su capacidad para manejar problemas de m√∫ltiples salidas o multioutput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a176f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.08      0.12        13\n",
      "           1       0.86      0.38      0.52        16\n",
      "           2       0.50      0.12      0.20         8\n",
      "           3       0.50      0.29      0.36        14\n",
      "           4       0.75      0.38      0.50         8\n",
      "           5       0.50      0.33      0.40        15\n",
      "           6       0.36      0.31      0.33        13\n",
      "           7       0.60      0.25      0.35        12\n",
      "           8       0.50      0.12      0.20         8\n",
      "           9       0.33      0.12      0.17        17\n",
      "\n",
      "   micro avg       0.51      0.24      0.33       124\n",
      "   macro avg       0.52      0.24      0.32       124\n",
      "weighted avg       0.51      0.24      0.32       124\n",
      " samples avg       0.36      0.26      0.28       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_target_classifier = RandomForestClassifier(n_estimators = 3, criterion = 'gini', n_jobs = 4, random_state = seed)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380064f",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225c0a6",
   "metadata": {},
   "source": [
    "El MLPClassifier, que representa el Perceptr√≥n Multicapa (Multilayer Perceptron) en la biblioteca scikit-learn, es una poderosa herramienta de aprendizaje supervisado utilizada para la clasificaci√≥n. Basado en redes neuronales artificiales, el MLPClassifier es conocido por su capacidad para modelar relaciones complejas entre las caracter√≠sticas de entrada y las etiquetas de salida en conjuntos de datos.\n",
    "\n",
    "Al aprovechar una arquitectura de red neuronal con m√∫ltiples capas de nodos interconectados, el MLPClassifier puede aprender patrones no lineales en los datos.\n",
    "\n",
    "M√°s informaci√≥n: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc9c39",
   "metadata": {},
   "source": [
    "Usamos el optimizador `Adam` ya que generalmente es el que mejores resultados ofrece. Las redes neuronales por construcci√≥n tambi√©n van a admitir f√°cilmente una clasificaci√≥n multi-output. `MLPClassifier` gestiona eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "723f59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.14        13\n",
      "           1       0.50      0.06      0.11        16\n",
      "           2       1.00      0.50      0.67         8\n",
      "           3       1.00      0.21      0.35        14\n",
      "           4       0.50      0.12      0.20         8\n",
      "           5       0.80      0.27      0.40        15\n",
      "           6       0.50      0.15      0.24        13\n",
      "           7       1.00      0.33      0.50        12\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       1.00      0.12      0.21        17\n",
      "\n",
      "   micro avg       0.79      0.18      0.29       124\n",
      "   macro avg       0.73      0.19      0.28       124\n",
      "weighted avg       0.76      0.18      0.28       124\n",
      " samples avg       0.30      0.18      0.21       124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "multi_target_classifier = MLPClassifier(activation = 'relu', solver = 'adam', max_iter = 1000, random_state = seed)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b54f8",
   "metadata": {},
   "source": [
    "#### K Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49fb7c",
   "metadata": {},
   "source": [
    "nota: no es multi-output de manera nativa por construcci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fa4991c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.20      0.06      0.10        16\n",
      "           2       0.50      0.25      0.33         8\n",
      "           3       0.00      0.00      0.00        14\n",
      "           4       0.00      0.00      0.00         8\n",
      "           5       1.00      0.07      0.12        15\n",
      "           6       0.50      0.31      0.38        13\n",
      "           7       0.33      0.25      0.29        12\n",
      "           8       1.00      0.38      0.55         8\n",
      "           9       0.44      0.24      0.31        17\n",
      "\n",
      "   micro avg       0.46      0.15      0.22       124\n",
      "   macro avg       0.40      0.15      0.21       124\n",
      "weighted avg       0.39      0.15      0.19       124\n",
      " samples avg       0.25      0.15      0.17       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_target_classifier = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd551adb",
   "metadata": {},
   "source": [
    "#### MultiOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628531d0",
   "metadata": {},
   "source": [
    "##### Regression Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "091125fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.23      0.35        13\n",
      "           1       1.00      0.25      0.40        16\n",
      "           2       0.64      0.88      0.74         8\n",
      "           3       0.71      0.36      0.48        14\n",
      "           4       0.43      0.38      0.40         8\n",
      "           5       0.67      0.27      0.38        15\n",
      "           6       0.43      0.23      0.30        13\n",
      "           7       0.75      0.50      0.60        12\n",
      "           8       0.00      0.00      0.00         8\n",
      "           9       0.91      0.59      0.71        17\n",
      "\n",
      "   micro avg       0.68      0.36      0.47       124\n",
      "   macro avg       0.63      0.37      0.44       124\n",
      "weighted avg       0.68      0.36      0.45       124\n",
      " samples avg       0.51      0.33      0.39       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = LogisticRegression(penalty = 'l2', solver = 'newton-cg', max_iter = 1000, multi_class = 'multinomial')\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "417a9432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.31      0.40        13\n",
      "           1       0.80      0.50      0.62        16\n",
      "           2       0.64      0.88      0.74         8\n",
      "           3       0.70      0.50      0.58        14\n",
      "           4       0.29      0.25      0.27         8\n",
      "           5       0.75      0.40      0.52        15\n",
      "           6       0.56      0.38      0.45        13\n",
      "           7       0.78      0.58      0.67        12\n",
      "           8       0.50      0.12      0.20         8\n",
      "           9       0.89      0.47      0.62        17\n",
      "\n",
      "   micro avg       0.67      0.44      0.53       124\n",
      "   macro avg       0.65      0.44      0.51       124\n",
      "weighted avg       0.68      0.44      0.52       124\n",
      " samples avg       0.58      0.46      0.48       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = PassiveAggressiveClassifier(random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "102daac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.31      0.36        13\n",
      "           1       0.80      0.25      0.38        16\n",
      "           2       0.47      0.88      0.61         8\n",
      "           3       0.56      0.36      0.43        14\n",
      "           4       0.14      0.12      0.13         8\n",
      "           5       0.86      0.40      0.55        15\n",
      "           6       0.42      0.38      0.40        13\n",
      "           7       0.60      0.75      0.67        12\n",
      "           8       0.40      0.25      0.31         8\n",
      "           9       0.75      0.53      0.62        17\n",
      "\n",
      "   micro avg       0.54      0.42      0.47       124\n",
      "   macro avg       0.54      0.42      0.45       124\n",
      "weighted avg       0.59      0.42      0.46       124\n",
      " samples avg       0.51      0.44      0.44       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = SGDClassifier(loss = 'squared_hinge', penalty = 'elasticnet', random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b696cc",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9781407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       1.00      0.44      0.61        16\n",
      "           2       1.00      0.12      0.22         8\n",
      "           3       1.00      0.36      0.53        14\n",
      "           4       1.00      0.12      0.22         8\n",
      "           5       1.00      0.27      0.42        15\n",
      "           6       0.60      0.23      0.33        13\n",
      "           7       1.00      0.33      0.50        12\n",
      "           8       1.00      0.12      0.22         8\n",
      "           9       1.00      0.18      0.30        17\n",
      "\n",
      "   micro avg       0.94      0.23      0.37       124\n",
      "   macro avg       0.86      0.22      0.34       124\n",
      "weighted avg       0.85      0.23      0.36       124\n",
      " samples avg       0.41      0.19      0.25       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = RandomForestClassifier(n_estimators = 10, random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5c074",
   "metadata": {},
   "source": [
    "##### Gradient Boosting: Grid Search (mejor modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "59e6add7-769c-40b6-931b-1fca0721a749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters: {'estimator__max_depth': 7, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 60}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.38      0.45        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       0.60      0.75      0.67         8\n",
      "           3       0.89      0.57      0.70        14\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      0.93      0.97        15\n",
      "           6       0.80      0.31      0.44        13\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       1.00      1.00      1.00        17\n",
      "\n",
      "   micro avg       0.90      0.77      0.83       124\n",
      "   macro avg       0.87      0.77      0.81       124\n",
      "weighted avg       0.89      0.77      0.81       124\n",
      " samples avg       0.88      0.80      0.81       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = GradientBoostingClassifier(loss = 'exponential', random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = -1)\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [40, 50, 60],\n",
    "    'estimator__max_depth': [3, 5, 7],\n",
    "    'estimator__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, average = 'weighted')\n",
    "\n",
    "grid_search = GridSearchCV(multi_target_classifier, param_grid, cv = 5, scoring = f1_scorer, n_jobs = -1, verbose = 1)\n",
    "grid_search.fit(x_train, y_train_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_estimator.fit(x_train, y_train_train)\n",
    "\n",
    "y_pred = best_estimator.predict(x_test)\n",
    "\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9846b1",
   "metadata": {},
   "source": [
    "Intentamos mejorar el modelo a√±adiendo un `AdaBoostClassifier` para mejorar el *fitting* del estimador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a4bdeabe-13e4-48f6-bc3c-0538b3394303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.46      0.60        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      0.75      0.86         8\n",
      "           3       0.80      0.57      0.67        14\n",
      "           4       1.00      1.00      1.00         8\n",
      "           5       1.00      0.93      0.97        15\n",
      "           6       1.00      0.38      0.56        13\n",
      "           7       1.00      0.92      0.96        12\n",
      "           8       0.88      0.88      0.88         8\n",
      "           9       1.00      1.00      1.00        17\n",
      "\n",
      "   micro avg       0.96      0.79      0.87       124\n",
      "   macro avg       0.95      0.79      0.85       124\n",
      "weighted avg       0.95      0.79      0.85       124\n",
      " samples avg       0.88      0.79      0.82       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(loss = 'exponential', criterion = 'friedman_mse', n_estimators = 50, max_depth = 5, min_samples_split = 5, random_state = seed)\n",
    "base_classifier = AdaBoostClassifier(estimator = estimator, n_estimators = 100, random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train, y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test)\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838794ad",
   "metadata": {},
   "source": [
    "##### Light Gradient-Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.54      0.60        26\n",
      "           1       1.00      0.74      0.85        35\n",
      "           2       0.77      0.77      0.77        13\n",
      "           3       0.81      0.76      0.79        29\n",
      "           4       1.00      0.87      0.93        23\n",
      "           5       1.00      0.89      0.94        36\n",
      "           6       0.56      0.48      0.52        31\n",
      "           7       1.00      0.84      0.91        25\n",
      "           8       0.96      0.92      0.94        25\n",
      "           9       1.00      1.00      1.00        35\n",
      "\n",
      "   micro avg       0.89      0.78      0.83       278\n",
      "   macro avg       0.88      0.78      0.82       278\n",
      "weighted avg       0.89      0.78      0.83       278\n",
      " samples avg       0.82      0.77      0.77       278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_classifier = lgb.LGBMClassifier(boosting_type = 'dart', n_estimators = 50, objective = 'binary', random_state = seed)\n",
    "multi_target_classifier = MultiOutputClassifier(base_classifier, n_jobs = 4)\n",
    "multi_target_classifier.fit(x_train.astype(np.float32), y_train_train)\n",
    "y_pred = multi_target_classifier.predict(x_test.astype(np.float32))\n",
    "report = classification_report(y_train_test, y_pred, zero_division = 0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7119e-b68d-447f-ae89-ef18c223ebf8",
   "metadata": {},
   "source": [
    "## Guardar predicciones de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_testArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89cd23",
   "metadata": {},
   "source": [
    "### Etiquetado de textos sin etiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe47a92",
   "metadata": {},
   "source": [
    "#### Asignaci√≥n de la etiqueta m√°s probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53288e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-labeled texts: [ 84  85  91  92 108 138 139 140 155 168 169 176 189]\n"
     ]
    }
   ],
   "source": [
    "zeroIdx = np.where(np.sum(y_test_pred, axis = 1) == 0)[0]\n",
    "if zeroIdx.size > 0:\n",
    "    print('Non-labeled texts:', zeroIdx)\n",
    "    probs = model.predict_proba(x_testArray)\n",
    "    for idx in zeroIdx:\n",
    "        oneProb = []\n",
    "        for probArray, classes in zip(probs, model.classes_):\n",
    "            p = probArray[idx][classes == 1][0]\n",
    "            oneProb.append(p)\n",
    "        y_test_pred[idx, np.argmax(oneProb)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ff4d4",
   "metadata": {},
   "source": [
    "#### Active-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainStack = x_train\n",
    "y_trainStack = y_train_train\n",
    "x_testStack = x_testArray\n",
    "y_predStack = y_test_pred\n",
    "\n",
    "zeroIdx = np.where(np.sum(y_predStack, axis = 1) == 0)[0]\n",
    "nZeros = []\n",
    "while zeroIdx.size > 0:\n",
    "\n",
    "    model.fit(x_trainStack, y_trainStack)\n",
    "    y_predStack = model.predict(x_testStack)\n",
    "\n",
    "    zeroIdx = np.where(np.sum(y_predStack, axis = 1) == 0)[0]\n",
    "    nZeros.append(zeroIdx.size)\n",
    "\n",
    "    print(x_trainStack.shape, x_testStack.shape, nZeros)\n",
    "\n",
    "    if len(nZeros) >= 3 and (nZeros[-1] == nZeros[-2] == nZeros[-3]):\n",
    "        raise Exception('Active Learning could not label any sample in the last 2 iterations. Quitting...')\n",
    "\n",
    "    x_lab = x_testStack[[i for i in range(x_testStack.shape[0]) if i not in zeroIdx]]\n",
    "    x_testStack = x_testStack[zeroIdx]\n",
    "\n",
    "    y_lab = y_predStack[[i for i in range(y_predStack.shape[0]) if i not in zeroIdx]]\n",
    "\n",
    "    x_trainStack = vstack([x_trainStack, x_lab])\n",
    "    y_trainStack = np.vstack([y_trainStack, y_lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775823c",
   "metadata": {},
   "source": [
    "### Escritura en fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff863c1c-9821-4ad4-a912-1abe400f5c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192 entries, 0 to 191\n",
      "Data columns (total 11 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   archivo  192 non-null    object\n",
      " 1   d49      192 non-null    int32 \n",
      " 2   i10      192 non-null    int32 \n",
      " 3   n28      192 non-null    int32 \n",
      " 4   r10      192 non-null    int32 \n",
      " 5   r11      192 non-null    int32 \n",
      " 6   r50      192 non-null    int32 \n",
      " 7   r52      192 non-null    int32 \n",
      " 8   r59      192 non-null    int32 \n",
      " 9   r60      192 non-null    int32 \n",
      " 10  r69      192 non-null    int32 \n",
      "dtypes: int32(10), object(1)\n",
      "memory usage: 9.1+ KB\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(y_test_pred, index = df_test.index, columns = clases)\n",
    "results.reset_index(inplace = True)\n",
    "\n",
    "results.to_csv('results/bow-multioutput-gradientboosting.csv', index = False)\n",
    "results.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
